# Stock Intelligence System - Architecture Analysis Report
**Analysis Date**: 2025-11-22
**Thoroughness Level**: Very Thorough
**Current System Status**: MVP Complete with Multi-LLM & Social Media Integration

---

## EXECUTIVE SUMMARY

The Stock Intelligence System is a **moderately complex** but **well-structured** application with solid architectural foundations. The system successfully integrates multiple data sources, implements proper async patterns, and has a clear separation of concerns. However, critical scalability challenges exist if the integration plan for 50+ new data sources is implemented without architectural changes.

### Key Finding
The current architecture can handle the proposed integration plan **IF** certain bottlenecks are addressed. Without mitigation, adding 50+ data sources could cause:
- Database connection pool exhaustion
- API endpoint response time degradation
- Task scheduling conflicts
- No prioritization mechanism for critical vs. non-critical collectors

---

## 1. CURRENT SYSTEM STRUCTURE

### 1.1 Directory Organization

```
/home/user/public-apis-4Kr/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI Application (1,765 lines)
â”‚   â”œâ”€â”€ config.py               # Configuration management
â”‚   â”œâ”€â”€ collectors/             # Data Collection Modules
â”‚   â”‚   â”œâ”€â”€ base.py            # Abstract base class
â”‚   â”‚   â”œâ”€â”€ kis_collector.py   # Korean Investment Securities API
â”‚   â”‚   â”œâ”€â”€ yahoo_collector.py # Yahoo Finance (US indices)
â”‚   â”‚   â”œâ”€â”€ dart_collector.py  # DART financial data
â”‚   â”‚   â”œâ”€â”€ news_collector.py  # News aggregation
â”‚   â”‚   â””â”€â”€ social_collector.py # Reddit/StockTwits
â”‚   â”œâ”€â”€ api/v1/                # API route organization (EMPTY - all routes in main.py)
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ orchestrator.py    # Multi-LLM coordination
â”‚   â”‚   â””â”€â”€ agents/            # Individual LLM agents
â”‚   â”‚       â”œâ”€â”€ base_agent.py
â”‚   â”‚       â”œâ”€â”€ claude_agent.py
â”‚   â”‚       â”œâ”€â”€ gpt4_agent.py
â”‚   â”‚       â”œâ”€â”€ gemini_agent.py
â”‚   â”‚       â””â”€â”€ grok_agent.py
â”‚   â”œâ”€â”€ models/                # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ stock.py          # Stock & financial data (8 tables)
â”‚   â”‚   â”œâ”€â”€ llm_analysis.py   # LLM tracking (4 tables)
â”‚   â”‚   â””â”€â”€ social_media.py   # Social data (2 tables)
â”‚   â”œâ”€â”€ analyzers/             # Technical analysis
â”‚   â”‚   â”œâ”€â”€ technical_analyzer.py
â”‚   â”‚   â”œâ”€â”€ signal_detector.py
â”‚   â”‚   â”œâ”€â”€ backtest_engine.py
â”‚   â”‚   â””â”€â”€ chart_ocr.py
â”‚   â”œâ”€â”€ recommenders/          # Recommendation engine
â”‚   â”œâ”€â”€ tasks/                 # Background jobs
â”‚   â”‚   â””â”€â”€ data_scheduler.py
â”‚   â”œâ”€â”€ database/              # DB session management
â”‚   â””â”€â”€ utils/                 # Logging, helpers
â”œâ”€â”€ dashboard/                 # Streamlit UI
â”‚   â”œâ”€â”€ app.py                 # Main dashboard
â”‚   â””â”€â”€ pages/                 # Multi-page structure
â””â”€â”€ tests/                     # Test suite
```

### 1.2 Entry Points

#### **FastAPI Application** (`/app/main.py`)
- **Type**: REST API server
- **Framework**: FastAPI with async support
- **Port**: 8000 (configurable)
- **Total Endpoints**: 33 routes
- **Deployment**: `uvicorn app.main:app --reload`

```python
# Code Snippet: FastAPI Initialization (lines 66-82)
app = FastAPI(
    title="Stock Intelligence System API",
    description="í•œêµ­ ì£¼ì‹ ìë™ë§¤ë§¤ ì§€ì› ì‹œìŠ¤í…œ API",
    version="1.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # âš ï¸ SECURITY: Should be restricted in production
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.on_event("startup")
async def startup_event():
    validate_config()
    init_db()
```

#### **Streamlit Dashboard** (`/dashboard/app.py`)
- **Type**: Web UI for data visualization
- **Port**: 8501 (default)
- **Features**: Market overview, stock lookup, data collection triggers
- **API Client**: Direct HTTP requests to FastAPI backend
- **Status Check**: Health check on `/health` endpoint

---

## 2. DATA COLLECTION FLOW

### 2.1 Collection Architecture

The system uses a **plugin-based collector pattern** with:
- Abstract `BaseCollector` class enforcing standard interface
- Async/await for non-blocking I/O
- Error handling with retry logic
- Metadata attachment for traceability

#### **Collector Hierarchy**

```
BaseCollector (abstract)
â”œâ”€â”€ KISCollector         # Korean Investment Securities API
â”œâ”€â”€ YahooCollector       # Yahoo Finance (US indices)
â”œâ”€â”€ DARTCollector        # DART financial disclosures
â”œâ”€â”€ NewsCollector        # News aggregation
â”œâ”€â”€ TradestieCollector   # Reddit/WSB trending
â””â”€â”€ StockTwitsCollector  # StockTwits sentiment
```

### 2.2 Detailed Data Flow Analysis

#### **2.2.1 KIS API (Korean Stock Real-Time Data)**

**File**: `/app/collectors/kis_collector.py`

**How It Works** (Synchronous â†’ Async pattern):

```python
# Code Snippet: KIS Token Management (lines 43-84)
async def get_access_token(self) -> str:
    """Get OAuth 2.0 access token with caching"""
    if self.access_token and self.token_expires_at:
        if datetime.now() < self.token_expires_at:
            return self.access_token  # âœ… Reuse valid token
    
    # Request new token (expires in 24 hours)
    url = f"{self.base_url}/oauth2/tokenP"
    data = {
        "grant_type": "client_credentials",
        "appkey": self.app_key,
        "appsecret": self.app_secret
    }
    
    async with aiohttp.ClientSession() as session:
        async with session.post(url, json=data) as response:
            result = await response.json()
            self.access_token = result['access_token']
            self.token_expires_at = datetime.now() + timedelta(hours=23)
            return self.access_token
```

**Data Collection Flow**:
1. **Token Management**: OAuth 2.0 token cached for 23 hours (expires in 24)
2. **API Call**: `GET /uapi/domestic-stock/v1/quotations/inquire-price`
3. **Parameters**: Market code (J=Stock), stock code (6 digits)
4. **Headers**: Authorization + App credentials
5. **Response**: JSON with price, volume, foreign ownership
6. **Async**: Uses `aiohttp` for non-blocking HTTP
7. **Metadata**: Adds source, timestamp, verification flag

**Status**: âœ… **Synchronous** (async-compatible but single request)
- No rate limiting implemented
- Single stock per call (batching needed for 50+ sources)
- No caching mechanism

#### **2.2.2 Yahoo Finance (US Indices)**

**File**: `/app/collectors/yahoo_collector.py`

**How It Works**:

```python
# Code Snippet: Yahoo Finance Collection (lines 44-106)
async def collect(self, symbol: str = "^GSPC", period: str = "3mo") -> Dict[str, Any]:
    """
    Collect US index data from Yahoo Finance
    Symbols: ^GSPC (S&P 500), ^IXIC (NASDAQ), ^DJI (Dow Jones)
    """
    try:
        ticker = yf.Ticker(symbol)
        hist = ticker.history(period=period)  # Download historical data
        
        # Latest data
        latest = hist.iloc[-1]
        
        # Calculate moving averages
        ma_20 = hist['Close'].rolling(window=20).mean().iloc[-1]
        ma_60 = hist['Close'].rolling(window=60).mean().iloc[-1]
        
        # Calculate change rate
        change_rate = ((latest['Close'] - hist.iloc[-2]['Close']) / 
                       hist.iloc[-2]['Close']) * 100
        
        return {
            'symbol': symbol,
            'name': index_name,
            'close': float(latest['Close']),
            'change_rate': change_rate,
            'ma_20': ma_20,
            'ma_60': ma_60,
            'above_ma': latest['Close'] > ma_20,
            'date': hist.index[-1].strftime('%Y-%m-%d')
        }
    except Exception as e:
        raise CollectionError(f"No data returned for symbol {symbol}")
```

**Status**: âœ… **Synchronous with Local Computation**
- Uses yfinance library (synchronous, but blocking)
- Computes moving averages locally
- No API key required
- Covers 3 months of history by default
- Used for US market signals in trading recommendations

#### **2.2.3 DART API (Financial Disclosures)**

**File**: `/app/collectors/dart_collector.py`

**How It Works**:

```python
# Code Snippet: DART Company Info (lines 37-78)
async def collect(self, corp_code: str) -> Dict[str, Any]:
    """
    Collect company basic information from DART
    corp_code: 8-digit unique identifier (e.g., '00005930' for Samsung)
    """
    url = f"{self.base_url}/company.json"
    params = {
        "crtfc_key": self.api_key,  # API Key from settings
        "corp_code": corp_code
    }
    
    async with aiohttp.ClientSession() as session:
        async with session.get(url, params=params) as response:
            if response.status != 200:
                raise CollectionError(f"Status {response.status}")
            
            result = await response.json()
            if result.get('status') != '000':  # DART uses status codes
                raise CollectionError(f"Error: {result.get('message')}")
            
            return result  # Returns company name, CEO, founding date, etc.
```

**Status**: âš ï¸ **Synchronous with Mapping Issues**
- Requires corp_code (8 digits), not stock_code (6 digits)
- No automatic mapping in current system
- Used for financial statements but integration incomplete
- **Critical Gap**: No batch collection method

#### **2.2.4 Social Media Collectors (Reddit/StockTwits)**

**File**: `/app/collectors/social_collector.py`

**WallStreetBets via Tradestie API**:

```python
# Code Snippet: Tradestie Collection (lines 35-78)
async def collect(self) -> List[Dict[str, Any]]:
    """
    Collect Top 50 stocks from WallStreetBets
    API: https://tradestie.com/api/v1/apps/reddit
    Rate Limit: 20 requests/minute (free tier)
    """
    async with aiohttp.ClientSession() as session:
        async with session.get(self.base_url, timeout=30) as response:
            if response.status == 200:
                data = await response.json()
                
                mentions = []
                for idx, item in enumerate(data):
                    mention = {
                        'source': 'wallstreetbets',
                        'platform': 'reddit',
                        'ticker': item.get('ticker'),
                        'mention_count': item.get('no_of_comments', 0),
                        'rank': idx + 1,
                        'sentiment': item.get('sentiment', 'NEUTRAL').upper(),
                        'sentiment_score': item.get('sentiment_score', 0.0),
                        'raw_data': item,
                        'data_date': datetime.now()
                    }
                    mentions.append(mention)
                
                return mentions
```

**StockTwits Sentiment Collection**:

```python
# Code Snippet: StockTwits Collection (lines 150-200 approx)
async def collect_symbol(self, symbol: str) -> Optional[Dict[str, Any]]:
    """
    Collect sentiment for specific ticker from StockTwits
    API: https://api.stocktwits.com/api/2/streams/symbol/{symbol}.json
    """
    url = f"{self.base_url}/{symbol.upper()}.json"
    
    async with aiohttp.ClientSession() as session:
        async with session.get(url, timeout=20) as response:
            if response.status == 200:
                data = await response.json()
                
                # Extract sentiment data from messages
                # Calculate bullish/bearish ratio
                # Return aggregated sentiment
```

**Status**: âœ… **Fully Async, Real-Time Capable**
- Both collectors use async/await
- Traded API: Free, no auth needed
- StockTwits: Public API, no rate limiting documented
- Saves to database with deduplication logic
- **Strength**: Handles multiple sources in parallel

### 2.3 Data Flow: API â†’ Database â†’ LLM

**Endpoint Flow Example**: `/api/v1/llm/analyze-signal/{stock_code}`

```
User Request
    â†“
[1] Collect Technical Data
    â”œâ”€â”€ Query StockPrice from DB (last 100 days)
    â””â”€â”€ Calculate indicators (RSI, MACD, MA)
    â†“
[2] Collect Market Context
    â”œâ”€â”€ Query USIndex for SP500 (latest)
    â””â”€â”€ Determine bullish/bearish signal
    â†“
[3] Collect Fundamental Data
    â”œâ”€â”€ Query Stock table for sector, market_cap
    â””â”€â”€ Query Financial table if available
    â†“
[4] Collect News Data
    â”œâ”€â”€ Query StockNews (last 7 days)
    â””â”€â”€ Aggregate sentiment
    â†“
[5] Run Multi-LLM Analysis (PARALLEL)
    â”œâ”€â”€ Claude Sonnet 4
    â”œâ”€â”€ GPT-4 Turbo
    â”œâ”€â”€ Gemini Pro
    â””â”€â”€ Grok 2
    â†“
[6] Calculate Consensus
    â”œâ”€â”€ Count BUY/SELL/HOLD votes
    â”œâ”€â”€ Calculate confidence
    â””â”€â”€ Determine agreement level
    â†“
[7] Save Results
    â”œâ”€â”€ LLMAnalysis table (4 records, one per model)
    â”œâ”€â”€ LLMConsensus table (1 record)
    â””â”€â”€ LLMPerformance table (update metrics)
    â†“
Return JSON response
```

**Code Implementation** (`main.py`, lines 1376-1496):

```python
@app.post("/api/v1/llm/analyze-signal/{stock_code}")
async def analyze_stock_signal_with_llm(stock_code: str, db: Session):
    # [1] Get stock info
    stock = db.query(Stock).filter(Stock.code == stock_code).first()
    
    # [2] Collect price history
    prices = db.query(StockPrice).filter(...).limit(100).all()
    
    # [3] Calculate technical data
    analyzer = TechnicalAnalyzer()
    technical_data = analyzer.calculate_all_indicators(price_df)
    
    # [4] Get US market signal
    sp500 = db.query(USIndex).filter(USIndex.symbol == '^GSPC').first()
    
    # [5] Run orchestrator
    orchestrator = LLMOrchestrator(db)
    result = await orchestrator.analyze_multi_agent(
        stock_code=stock_code,
        technical_data=technical_data,
        ...
    )
    
    return {"status": "success", "data": result}
```

---

## 3. LLM INTEGRATION: MULTI-AGENT CONSENSUS

### 3.1 Current Implementation

**File**: `/app/llm/orchestrator.py`

**Architecture**:
- 4 LLM agents (Claude, GPT-4, Gemini, Grok) run in PARALLEL
- Each agent independently analyzes stock data
- Results aggregated via voting consensus mechanism
- Performance tracked for model selection optimization

### 3.2 LLM Agent Implementation

**Base Agent Class** (`/app/llm/agents/base_agent.py`):

```python
# Code Snippet: Base Agent Interface (lines 14-89)
class BaseLLMAgent(ABC, LoggerMixin):
    """Abstract base for all LLM agents"""
    
    def __init__(self, api_key: Optional[str] = None, model_name: str = "unknown"):
        self.api_key = api_key
        self.model_name = model_name
        self.request_count = 0
        self.total_tokens = 0
        self.total_cost = 0.0
    
    @abstractmethod
    async def analyze_news_risk(self, news_data: Dict) -> Dict[str, Any]:
        """Analyze news and assess risks"""
        pass
    
    @abstractmethod
    async def generate_combined_signal(
        self, 
        technical_data: Dict,
        fundamental_data: Dict,
        us_market_data: Dict,
        news_data: Dict
    ) -> Dict[str, Any]:
        """Generate trading signal: BUY/SELL/HOLD"""
        pass
    
    @abstractmethod
    async def explain_for_beginner(
        self,
        stock_code: str,
        stock_name: str,
        analysis_data: Dict
    ) -> str:
        """Beginner-friendly explanation in Korean"""
        pass
```

### 3.3 Claude Agent Example

**File**: `/app/llm/agents/claude_agent.py`

```python
# Code Snippet: Claude API Integration (lines 36-81)
async def _make_api_call(
    self,
    prompt: str,
    system_prompt: Optional[str],
    max_tokens: int,
    temperature: float
) -> Dict[str, Any]:
    """Make API call to Claude"""
    import anthropic
    
    client = anthropic.Anthropic(api_key=self.api_key)
    
    message = client.messages.create(
        model=self.model_id,              # 'claude-sonnet-4-20250514'
        max_tokens=max_tokens,            # Up to 4000 tokens
        temperature=temperature,
        system=system_prompt or "You are a professional stock market analyst.",
        messages=[
            {"role": "user", "content": prompt}
        ]
    )
    
    content = message.content[0].text
    tokens_used = message.usage.input_tokens + message.usage.output_tokens
    
    # Cost calculation (Sonnet pricing)
    cost = (message.usage.input_tokens * 0.003 / 1000) + \
           (message.usage.output_tokens * 0.015 / 1000)
    
    return {
        'content': content,
        'tokens_used': tokens_used,
        'cost': cost
    }
```

**Status**: âœ… **Async-compatible, Mock responses on API failure**

### 3.4 Multi-Agent Orchestration

**File**: `/app/llm/orchestrator.py` (lines 52-144)

```python
# Code Snippet: Parallel LLM Execution (lines 162-192)
async def _run_combined_signal_analysis(
    self,
    stock_code: str,
    stock_name: str,
    technical_data: Optional[Dict],
    fundamental_data: Optional[Dict],
    us_market_data: Optional[Dict],
    news_data: Optional[Dict]
) -> Dict[str, Dict]:
    """Run combined signal analysis with all agents"""
    
    # Create tasks for parallel execution
    tasks = {
        'claude': self.claude.generate_combined_signal(
            technical_data, fundamental_data, us_market_data, news_data
        ),
        'gpt4': self.gpt4.generate_combined_signal(
            technical_data, fundamental_data, us_market_data, news_data
        ),
        'gemini': self.gemini.generate_combined_signal(
            technical_data, fundamental_data, us_market_data, news_data
        ),
        'grok': self.grok.generate_combined_signal(
            technical_data, fundamental_data, us_market_data, news_data
        )
    }
    
    # Run all in parallel using asyncio.gather
    results = await asyncio.gather(*tasks.values(), return_exceptions=True)
    
    return {
        model: result if not isinstance(result, Exception) else {'success': False, 'error': str(result)}
        for model, result in zip(tasks.keys(), results)
    }
```

### 3.5 Consensus Voting Mechanism

**File**: `/app/llm/orchestrator.py` (lines 227-291)

```python
# Code Snippet: Consensus Calculation (lines 238-291)
def _calculate_consensus(self, results: Dict[str, Dict], analysis_type: str) -> Dict[str, Any]:
    """
    Calculate consensus from multiple LLM results
    """
    votes = {'BUY': 0, 'SELL': 0, 'HOLD': 0}
    confidences = []
    successful_models = []
    
    # Count votes
    for model_name, result in results.items():
        if result.get('success') and result.get('decision'):
            decision = result['decision'].upper()
            if decision in votes:
                votes[decision] += 1
                confidences.append(result.get('confidence', 0))
                successful_models.append(model_name)
    
    total_models = len(successful_models)
    
    if total_models == 0:
        return {
            'decision': 'NO_CONSENSUS',
            'confidence': 0.0,
            'agreement_level': 0.0,
            'votes': votes
        }
    
    # Determine consensus decision (majority vote)
    max_votes = max(votes.values())
    consensus_decision = [k for k, v in votes.items() if v == max_votes][0]
    
    # Calculate agreement level
    agreement_level = max_votes / total_models
    
    # Adjust confidence based on agreement strength
    if agreement_level >= 0.75:  # 3/4 or 4/4 agree
        consensus_confidence = sum(confidences) / len(confidences)
        consensus_strength = "STRONG"
    elif agreement_level >= 0.5:   # 2/4 agree
        consensus_confidence = (sum(confidences) / len(confidences)) * 0.7
        consensus_strength = "MODERATE"
    else:
        consensus_decision = 'NO_CONSENSUS'
        consensus_confidence = 0.0
        consensus_strength = "WEAK"
    
    return {
        'decision': consensus_decision,
        'confidence': round(consensus_confidence, 2),
        'agreement_level': round(agreement_level, 2),
        'strength': consensus_strength,
        'votes': votes,
        'successful_models': total_models
    }
```

**Voting Logic**:
- Simple majority vote (plurality rule)
- 4/4 agreement = STRONG (full confidence)
- 3/4 agreement = STRONG (full confidence)
- 2/4 agreement = MODERATE (70% of stated confidence)
- 1/4 or 0/4 = NO_CONSENSUS (0% confidence)

### 3.6 Existing Orchestration Patterns

**Current Orchestration**:
1. âœ… **Parallel execution** of 4 agents using asyncio.gather()
2. âœ… **Error handling** with fallback to mock responses
3. âœ… **Result persistence** to LLMAnalysis, LLMConsensus, LLMPerformance tables
4. âš ï¸ **No dynamic routing** (all agents always called)
5. âš ï¸ **No cost optimization** (expensive models called equally)
6. âš ï¸ **No cascading** (if one fails, continues anyway)

**Buildable Infrastructure**:
- âœ… Database schema for tracking decisions
- âœ… Performance metrics collection
- âœ… Cost tracking per model
- âœ… Latency measurement
- âœ… Token usage monitoring

---

## 4. SOCIAL MEDIA INTEGRATION

### 4.1 Current Implementation

**File**: `/app/collectors/social_collector.py`

**Two Independent Collectors**:

1. **Tradestie API** (WallStreetBets)
   - Source: https://tradestie.com/api/v1/apps/reddit
   - Data: Top 50 stocks from r/wallstreetbets
   - Rate Limit: 20 requests/minute
   - Cost: FREE
   - Sentiment: Provided by API

2. **StockTwits API** 
   - Source: https://api.stocktwits.com/api/2/streams/symbol/{symbol}.json
   - Data: Ticker sentiment, bullish/bearish ratio
   - Rate Limit: Not documented (assumed generous)
   - Cost: FREE
   - Sentiment: Calculated from messages

### 4.2 Data Integration Flow

**Endpoint**: `POST /api/v1/social/collect`

```python
# Code Snippet: Unified Collection (lines 1501-1527 in main.py)
@app.post("/api/v1/social/collect")
async def collect_social_media_data(db: Session):
    """Collect from WallStreetBets + StockTwits"""
    try:
        # This calls collect_all_social_data() from social_collector.py
        results = await collect_all_social_data(db)
        
        return {
            "status": "success",
            "data": {
                "wallstreetbets_mentions": results.get('wallstreetbets', 0),
                "stocktwits_mentions": results.get('stocktwits', 0),
                "total_collected": sum(results.values()),
                "timestamp": datetime.now().isoformat()
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### 4.3 Database Schema

**Table**: `social_media_mentions`

```python
# From models/social_media.py
class SocialMediaMention(Base):
    __tablename__ = "social_media_mentions"
    
    id = Column(Integer, primary_key=True)
    source = Column(String(50))       # 'wallstreetbets', 'stocktwits'
    platform = Column(String(50))     # 'reddit', 'stocktwits'
    ticker = Column(String(20))       # 'TSLA', 'AAPL', etc.
    stock_code = Column(String(10))   # Korean mapping (future)
    mention_count = Column(Integer)
    rank = Column(Integer)            # Ranking within source
    sentiment = Column(String(20))    # 'BULLISH', 'BEARISH', 'NEUTRAL'
    sentiment_score = Column(Float)   # -1.0 to 1.0
    bullish_ratio = Column(Float)     # 0.0 to 1.0 (StockTwits)
    raw_data = Column(JSON)           # Full API response
    data_date = Column(DateTime)      # Data collection date
    collected_at = Column(DateTime)   # When collected
```

### 4.4 Integration with Market Data

**Endpoints**:

1. **`GET /api/v1/social/wallstreetbets/trending`**
   - Returns top 20 stocks from WSB
   - Deduplication: Same day, same source, same ticker
   - Sorting: By rank (descending)

2. **`GET /api/v1/social/stocktwits/{ticker}`**
   - Returns sentiment for specific ticker
   - Real-time collection if not in DB
   - Includes sentiment breakdown

3. **`GET /api/v1/social/trending-combined`**
   - **Integrates both sources**
   - Scores: WSB mention count (1.0) + StockTwits bullish ratio (100)
   - Returns unified trending list

**Code Example** (`main.py`, lines 1683-1752):

```python
@app.get("/api/v1/social/trending-combined")
async def get_combined_social_trends(limit: int = 30, db: Session):
    """Combined trending from WSB + StockTwits"""
    all_mentions = db.query(SocialMediaMention).filter(...).all()
    
    # Group by ticker
    ticker_data = {}
    for mention in all_mentions:
        ticker = mention.ticker
        if ticker not in ticker_data:
            ticker_data[ticker] = {
                'ticker': ticker,
                'wsb_rank': None,
                'wsb_mentions': 0,
                'wsb_sentiment': None,
                'stocktwits_sentiment': None,
                'combined_score': 0
            }
        
        if mention.source == 'wallstreetbets':
            ticker_data[ticker]['wsb_rank'] = mention.rank
            ticker_data[ticker]['wsb_mentions'] = mention.mention_count
            ticker_data[ticker]['wsb_sentiment'] = mention.sentiment
            ticker_data[ticker]['combined_score'] += mention.mention_count * 1.0
        elif mention.source == 'stocktwits':
            ticker_data[ticker]['stocktwits_sentiment'] = mention.sentiment
            ticker_data[ticker]['stocktwits_bullish_ratio'] = mention.bullish_ratio
            ticker_data[ticker]['combined_score'] += (mention.bullish_ratio or 0.5) * 100
    
    sorted_data = sorted(ticker_data.values(), ...)[:limit]
    return {"status": "success", "data": {"trending_stocks": sorted_data}}
```

---

## 5. API GATEWAY STRUCTURE

### 5.1 Endpoint Inventory (33 Total)

**By Category**:

| Category | Count | Endpoints |
|----------|-------|-----------|
| **Stock Data** | 3 | List, Details, Collect |
| **US Market** | 2 | Get, Collect |
| **Market Overview** | 1 | Overview |
| **Recommendations** | 3 | Analyze Profile, Get, History |
| **Trading Signals** | 3 | US Market, Stock, Combined |
| **Backtesting** | 2 | Run, Results |
| **Sectors** | 3 | All, Beginner-friendly, Details |
| **Chart Analysis** | 1 | Analyze Chart |
| **News & Sentiment** | 2 | News, Sentiment |
| **LLM Analysis** | 5 | Multi-agent, Consensus, Performance, History, Signal |
| **Social Media** | 4 | Collect, WSB Trending, StockTwits, Combined |
| **System** | 2 | Health, Root |
| **Events** | 2 | Startup, Shutdown |

### 5.2 Current API Organization

**Status**: âš ï¸ **MONOLITHIC** - All routes in `/app/main.py`

```
app/
â”œâ”€â”€ api/
â”‚   â””â”€â”€ v1/
â”‚       â””â”€â”€ __init__.py   # Empty file - no actual route organization
â””â”€â”€ main.py              # 1,765 lines with ALL 33 endpoints
```

**Architectural Problem**: 
- No separation by feature/domain
- Hard to maintain and test
- Scales poorly beyond 50+ endpoints
- All imports in single file

### 5.3 Key Endpoint Examples

**Data Collection Endpoint**:
```python
@app.post("/api/v1/stocks/{stock_code}/collect")
async def collect_stock_data(stock_code: str, db: Session):
    """
    Trigger real-time data collection for specific stock
    Flow: KISCollector.collect() â†’ validate â†’ save to DB â†’ return
    """
    collector = KISCollector()
    data = await collector.safe_collect(stock_code=stock_code)
    # ... save to StockPrice table
```

**LLM Analysis Endpoint**:
```python
@app.post("/api/v1/llm/analyze-multi")
async def analyze_with_multi_llm(request: LLMAnalyzeRequest, db: Session):
    """
    Run multi-agent analysis with 4 LLMs in parallel
    Flow: Collect data â†’ Run 4 agents â†’ Calculate consensus â†’ Save results
    """
    orchestrator = LLMOrchestrator(db)
    result = await orchestrator.analyze_multi_agent(...)
```

### 5.4 Routing Pattern

**Current Pattern**: Direct to function in main.py
```
Request â†’ FastAPI Router â†’ Main.py endpoint â†’ Service logic
```

**Observed Routing Issues**:
1. âœ… RESTful naming conventions
2. âœ… Proper HTTP methods (GET/POST)
3. âš ï¸ No request validation (Pydantic used minimally)
4. âš ï¸ No response standardization
5. âš ï¸ No API versioning support beyond URL `/v1/`

---

## 6. DATABASE INTEGRATION

### 6.1 Database Schema (14 Tables)

**File**: `/app/models/` (multiple files)

#### **Stock Domain Tables** (8 tables)

1. **stocks** - Master stock table
   - Columns: code (PK), name, market, sector, market_cap, description
   - Indexes: sector, market_cap

2. **stock_prices** - Daily OHLCV data
   - Columns: id, stock_code (FK), date, open, high, low, close, volume, change_rate, foreign_ownership
   - Indexes: (stock_code, date), date

3. **financials** - Financial statements
   - Columns: id, stock_code (FK), year, quarter, revenue, net_income, PER, PBR, ROE, etc.
   - Indexes: (stock_code, year)

4. **us_indices** - S&P 500, NASDAQ, Dow Jones
   - Columns: id, symbol, date, close, change_rate, ma_20, ma_60, above_ma
   - Indexes: (symbol, date)

5. **economic_indicators** - ECOS, FRED data
   - Columns: id, indicator_name, country, value, unit, date, source
   - Indexes: (indicator_name, date)

6. **stock_news** - News articles with sentiment
   - Columns: id, stock_code (FK), title, content, source, sentiment_score, sentiment_label, published_at
   - Indexes: (stock_code, published_at)

7. **recommendations** - System recommendations
   - Columns: id, stock_code (FK), score, risk_level, reasons (JSONB), expected_return_1m, valid_until
   - Indexes: score, created_at

8. **backtest_results** - Backtest execution results
   - Columns: id, strategy_name, start_date, end_date, total_return, cagr, mdd, sharpe_ratio, parameters (JSONB)
   - Indexes: strategy_name, sharpe_ratio

#### **LLM Analysis Tables** (4 tables)

9. **llm_analyses** - Individual LLM agent results
   - Columns: id, stock_code, analysis_type, llm_model, decision, confidence, tokens_used, cost, latency_ms, success
   - Indexes: stock_code, llm_model, analysis_type

10. **llm_consensus** - Multi-LLM consensus results
    - Columns: id, stock_code, consensus_decision, consensus_confidence, agreement_level, buy_votes, sell_votes, hold_votes
    - Indexes: stock_code, created_at

11. **llm_performance** - Model performance metrics
    - Columns: id, llm_model (unique), total_requests, total_tokens, total_cost, avg_confidence, accuracy_score
    - Indexes: llm_model

12. **data_collection_log** - Collection audit trail
    - Columns: id, collector_name, status, items_collected, items_failed, execution_time_ms, created_at
    - Indexes: collector_name, created_at

#### **Social Media Tables** (2 tables)

13. **social_media_mentions** - Reddit/StockTwits data
    - Columns: id, source, platform, ticker, mention_count, rank, sentiment, sentiment_score, bullish_ratio, raw_data (JSON), data_date
    - Indexes: source, ticker, data_date

14. **social_influencer_posts** - Influencer tracking (future)
    - Columns: id, username, platform, post_id, mentioned_tickers (JSON), sentiment, like_count, retweet_count, impact_score
    - Indexes: username, posted_at

### 6.2 Data Flow: API â†’ Database

**Example**: Endpoint `/api/v1/stocks/{stock_code}/collect`

```
1. API receives request with stock_code
2. KISCollector.collect(stock_code) â†’ HTTP GET to KIS API
3. Parse response â†’ StockPrice object
4. db.add(StockPrice)
5. db.commit()
6. Return response

Tables affected:
â”œâ”€â”€ stocks (read for metadata)
â””â”€â”€ stock_prices (insert new record)
```

### 6.3 Database Session Management

**File**: `/app/database/session.py`

```python
# Code Snippet: Session Configuration (lines 16-36)
database_url = get_database_url()  # Priority: SUPABASE_DB_URL > DATABASE_URL

engine = create_engine(
    database_url,
    pool_size=settings.DB_POOL_SIZE,       # Default: 5
    max_overflow=settings.DB_MAX_OVERFLOW, # Default: 10
    pool_pre_ping=True,                    # Verify before use
    echo=settings.DEBUG,                   # Log SQL in debug mode
)

SessionLocal = sessionmaker(
    autocommit=False,
    autoflush=False,
    bind=engine
)
```

**Connection Pool Configuration**:
- Pool Size: 5 (default)
- Max Overflow: 10
- **Total capacity**: 15 concurrent connections
- **âš ï¸ Critical**: For 50+ parallel collectors, this is INSUFFICIENT

### 6.4 Data Consistency & Verification

**Deduplication Strategy** (in collectors):

```python
# Code Example: Avoiding duplicate social media records
existing = db.query(SocialMediaMention).filter(
    SocialMediaMention.source == mention_data['source'],
    SocialMediaMention.ticker == mention_data['ticker'],
    SocialMediaMention.data_date >= datetime.now().replace(hour=0, minute=0, second=0)
).first()

if existing:
    # Update existing record
    existing.mention_count = mention_data['mention_count']
    db.commit()
else:
    # Create new record
    mention = SocialMediaMention(**mention_data)
    db.add(mention)
    db.commit()
```

---

## 7. CRITICAL ASSESSMENT

### 7.1 Existing Orchestration Patterns

#### âœ… **What's Already Built**

1. **Parallel LLM Execution**
   - Uses `asyncio.gather(*tasks.values())`
   - All 4 agents called simultaneously
   - Result aggregation with consensus voting
   - **Can build on this for dynamic routing**

2. **Async Data Collection**
   - BaseCollector pattern supports async
   - Social media collectors fully async
   - Non-blocking HTTP via aiohttp
   - **Ready for burst loads**

3. **Result Persistence**
   - LLMAnalysis, LLMConsensus, LLMPerformance tables
   - Cost tracking per model
   - Latency measurement per agent
   - **Performance data available for optimization**

4. **Error Handling**
   - Try/except with fallback to mock responses
   - Individual agent failures don't block consensus
   - Database rollback on errors
   - **Resilient to partial failures**

#### âš ï¸ **Gaps & Limitations**

1. **No Dynamic Collector Orchestration**
   - All collectors always called with equal priority
   - No cost-benefit analysis
   - No cascading or conditional execution
   - **Solution needed**: Rate limiting, priority queues

2. **Database Connection Pooling Undersized**
   - Pool size: 5, overflow: 10 = 15 total
   - 50+ collectors = connection exhaustion
   - **Critical bottleneck** for scaling

3. **No Request Deduplication**
   - Same data collected multiple times
   - No caching layer (Redis configured but unused)
   - **Inefficient for overlapping data needs**

4. **Scheduler Not Implemented**
   - DataScheduler class exists but not integrated
   - No cron-like execution
   - Manual endpoint calls only
   - **Cannot sustain "every 30 min" collection**

5. **No Result Caching**
   - Every endpoint query hits database
   - No Redis integration for hot data
   - Analysis results not cached
   - **Response times will degrade with scale**

### 7.2 What Would Break with 50+ New Data Sources

#### **Problem 1: Database Connection Pool Exhaustion**

```
Current: pool_size=5, max_overflow=10
Scenario: 50 collectors Ã— 10 stocks each = 500 concurrent requests

Event flow:
â”œâ”€â”€ Request 1-5: Use pooled connections âœ…
â”œâ”€â”€ Request 6-15: Use overflow connections âœ…
â”œâ”€â”€ Request 16+: WAIT or TIMEOUT âŒ
â””â”€â”€ Result: Cascading failures, timeouts
```

**Impact**: Data collection stalls, endpoints hang

#### **Problem 2: API Response Time Degradation**

```
Current: Sequential database queries per endpoint
With 50 sources: Cache misses increase exponentially

Example: /api/v1/stocks/{code}
â”œâ”€â”€ Query stocks: 1ms
â”œâ”€â”€ Query stock_prices: 5ms
â”œâ”€â”€ Query all 50 data sources: 50 Ã— 5ms = 250ms+ âŒ
â””â”€â”€ Total response time: 250ms+ (vs current 50ms)
```

**Impact**: Slow dashboard, poor UX, timeout errors

#### **Problem 3: Task Scheduling Conflicts**

```
Current: No scheduler
Proposed: "Collect from source X every 30 minutes"

Without orchestration:
â”œâ”€â”€ Schedule A: "Collect source 1"
â”œâ”€â”€ Schedule B: "Collect source 1" (duplicate)
â”œâ”€â”€ ...
â””â”€â”€ Result: Wasted API calls, duplicate DB records
```

**Impact**: Resource waste, inconsistent data

#### **Problem 4: No Cost/Benefit Prioritization**

```
Current: All sources equally valued
Scenario: 50 sources, limited budget

Without prioritization:
â”œâ”€â”€ Expensive GPT-4 called equally with free sources
â”œâ”€â”€ Rarely-updated sources queried frequently
â”œâ”€â”€ High-variance data prioritized equally with stable data
â””â”€â”€ Result: Wasted compute budget, poor signal quality
```

**Impact**: Higher operational costs, diminishing returns

#### **Problem 5: No Request Rate Limiting**

```
Current: No rate limiter on collector endpoints
Scenario: 50 collectors, each with different rate limits

Without coordination:
â”œâ”€â”€ Tradestie API: 20/min â†’ bombarded with 100+/min âŒ
â”œâ”€â”€ StockTwits: Unknown limits â†’ throttled âŒ
â”œâ”€â”€ KIS API: 1000/day â†’ exhausted by 50 collectors âŒ
â””â”€â”€ Result: IP bans, API key revocations
```

**Impact**: Service degradation, lost data sources

### 7.3 Architectural Scalability Assessment

#### **Can Current Architecture Support 50+ Sources?**

**Short Answer**: âš ï¸ **Not without significant refactoring**

**By Component**:

| Component | Current | With 50+ Sources | Verdict |
|-----------|---------|------------------|---------|
| **Data Collection** | âœ… Async-ready | âš ï¸ Scheduling needed | Possible |
| **LLM Orchestration** | âœ… Parallel | âœ… Can handle | âœ“ OK |
| **Database** | âš ï¸ Pool too small | âŒ Will fail | NEEDS FIX |
| **API Routing** | âš ï¸ Monolithic | âŒ Unmaintainable | NEEDS REFACTOR |
| **Caching** | âŒ Unused | âš ï¸ Critical need | NEEDS IMPL |
| **Cost Tracking** | âœ… Per-model | âš ï¸ Need per-source | NEEDS EXTEND |

### 7.4 Actual Bottlenecks

**Ranked by Impact** (if adding 50+ sources):

1. **Database Connection Pool** (CRITICAL)
   - Current: 5 + 10 overflow = 15 max connections
   - Needed: ~50-100 for safe concurrent operations
   - Fix: `pool_size=25, max_overflow=50` (minimum)

2. **No Request Scheduling/Queue** (HIGH)
   - Current: Ad-hoc collection via endpoints
   - Needed: Priority queue with rate limiting
   - Solution: Celery + Redis or APScheduler with backpressure

3. **No Response Caching** (HIGH)
   - Current: Every request hits database
   - Needed: Redis cache for hot data (1hr TTL)
   - Solution: Add caching layer via FastAPI

4. **Monolithic API Routing** (MEDIUM)
   - Current: All routes in main.py
   - Needed: Feature-based modules
   - Solution: Split into routers (collectors/, analysis/, social/)

5. **No Collector Prioritization** (MEDIUM)
   - Current: All sources treated equally
   - Needed: Dynamic weighting by value/cost
   - Solution: Collector metadata with weights

### 7.5 Cost Analysis for Proposed Integration

**Running 50+ data sources continuously**:

```
Collectors breakdown:
â”œâ”€â”€ Free (KIS, Yahoo, DART, ECOS): ~10 sources âœ…
â”œâ”€â”€ Rate-limited free (Reddit, StockTwits): ~5 sources âœ…
â”œâ”€â”€ Requires credentials: ~35 sources (estimated)
â”‚   â”œâ”€â”€ Financial data: Bloomberg, Refinitiv, etc.
â”‚   â”œâ”€â”€ Alternative data: Crypto exchanges, commodity APIs
â”‚   â””â”€â”€ News: NewsAPI, Newsdata.io, SerpAPI
â””â”€â”€ Total estimated monthly: $500-2000 USD

Cost drivers:
â”œâ”€â”€ LLM analysis: $0.01-0.05 per analysis Ã— 100/day = $1-5/day = $30-150/month
â”œâ”€â”€ Data APIs: $10-100/month (varies by source)
â”œâ”€â”€ Compute: $100-300/month (AWS Lambda/EC2)
â””â”€â”€ Database: $50-200/month (Supabase PostgreSQL)

Total: ~$200-650/month
```

### 7.6 Feasibility Verdict

#### **Can You Add 50+ Data Sources?**

**YES, but with these conditions**:

1. âœ… **Increase database pool**: 25 base + 50 overflow minimum
2. âœ… **Implement task scheduler**: Celery or APScheduler with rate limiting
3. âœ… **Add response caching**: Redis with smart TTL per source type
4. âœ… **Refactor API routes**: Split main.py into modules
5. âœ… **Implement cost controls**: Per-source budgeting and priority weighting
6. âœ… **Build orchestration layer**: Smart routing based on value/cost/latency

#### **Timeline Estimate**:

```
Week 1-2: Database & pooling fixes
Week 3-4: Task scheduling + rate limiting
Week 5-6: Caching layer + API refactoring
Week 7-8: Orchestration & cost controls
Week 9: Testing + integration
Week 10-12: Monitoring + optimization

Total: 10-12 weeks for production-ready 50+ source system
```

---

## 8. RECOMMENDATIONS

### 8.1 Immediate Actions (Before Adding Sources)

1. **Increase Database Connection Pool**
   ```python
   # app/config.py - change defaults
   DB_POOL_SIZE: int = Field(default=25, description="...")
   DB_MAX_OVERFLOW: int = Field(default=50, description="...")
   ```

2. **Implement Redis Caching**
   ```python
   # Add to main.py startup
   redis_client = redis.Redis(...)
   cache_ttl = 3600  # 1 hour
   ```

3. **Add Request Rate Limiting**
   ```python
   # Add to dependencies
   from slowapi import Limiter
   limiter = Limiter(key_func=get_remote_address)
   app.state.limiter = limiter
   ```

4. **Split Monolithic main.py**
   ```
   app/api/v1/
   â”œâ”€â”€ stocks.py       # Stock endpoints
   â”œâ”€â”€ llm.py          # LLM endpoints  
   â”œâ”€â”€ social.py       # Social endpoints
   â””â”€â”€ __init__.py
   ```

### 8.2 Medium-term Improvements

1. **Implement APScheduler**
   ```python
   scheduler = APScheduler()
   scheduler.add_job(collect_us_market, 'cron', hour='9')
   scheduler.add_job(collect_social, 'interval', minutes=30)
   ```

2. **Add Collector Metadata**
   ```python
   COLLECTOR_CONFIG = {
       'kis': {'priority': 1, 'cost': 0, 'rate_limit': 1000/day},
       'yahoo': {'priority': 2, 'cost': 0, 'rate_limit': None},
       'stocktwits': {'priority': 1, 'cost': 0, 'rate_limit': 100/min},
   }
   ```

3. **Extend LLM Cost Tracking**
   ```python
   # Track per-source cost, not just per-model
   collector_cost = CollectorCostTracking(
       source='kis',
       api_calls=100,
       tokens=5000,
       cost=0.50
   )
   ```

### 8.3 Long-term Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         API Gateway (FastAPI)           â”‚
â”‚  â”œâ”€ /api/v1/stocks/                    â”‚
â”‚  â”œâ”€ /api/v1/llm/                       â”‚
â”‚  â””â”€ /api/v1/social/                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Request Cache (Redis)                   â”‚
â”‚  (1hr TTL per source type)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestration Layer                     â”‚
â”‚  â”œâ”€ Rate Limiter (per source)           â”‚
â”‚  â”œâ”€ Priority Queue                      â”‚
â”‚  â””â”€ Cost Budgeter                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Collectors (50+ instances)              â”‚
â”‚  â”œâ”€ Financial APIs                      â”‚
â”‚  â”œâ”€ Social Media APIs                   â”‚
â”‚  â””â”€ Market Data APIs                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data Persistence (Supabase)             â”‚
â”‚  â””â”€ 14+ tables with proper indexing     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## CONCLUSION

The Stock Intelligence System has **strong foundations** for integration with additional data sources:

### âœ… **Strengths**
- Well-designed LLM orchestration (parallelizable, extensible)
- Proper async patterns throughout
- Clear collector interface for new sources
- Comprehensive database schema
- Cost and performance tracking built in

### âš ï¸ **Critical Gaps**
- Database connection pool too small (15 vs. needed 75+)
- No request scheduling or rate limiting
- No response caching layer
- Monolithic API structure
- No cost-based source prioritization

### ğŸ“Š **Verdict on Your Plan**
**Is integration of 50+ sources realistic?** 

âœ… **YES** - Architecturally feasible with medium effort
â±ï¸ **Timeline**: 10-12 weeks for production-ready system
ğŸ’° **Additional Cost**: $200-650/month for APIs + compute
ğŸ”§ **Effort**: 3-4 developer-weeks of refactoring + testing

The system is not "overly optimistic" but requires addressing the connection pool and scheduling bottlenecks before scaling.

---

## APPENDIX: Key Files Reference

| File | Lines | Purpose |
|------|-------|---------|
| `/app/main.py` | 1,765 | All 33 API endpoints |
| `/app/llm/orchestrator.py` | ~400 | Multi-LLM coordination |
| `/app/collectors/base.py` | 160 | Collector interface |
| `/app/database/session.py` | 97 | DB connection management |
| `/app/models/stock.py` | 236 | Stock-related tables |
| `/app/models/llm_analysis.py` | ~150 | LLM tracking tables |
| `/app/models/social_media.py` | 95 | Social media tables |
| `/dashboard/app.py` | 363 | Streamlit interface |

